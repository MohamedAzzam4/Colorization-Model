{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1913658,"sourceType":"datasetVersion","datasetId":1036526}],"dockerImageVersionId":30121,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<th><Name></th>","metadata":{}},{"cell_type":"markdown","source":"<th><Name></th>","metadata":{}},{"cell_type":"markdown","source":"## <font color = green> Import Libraries</font>","metadata":{}},{"cell_type":"code","source":"#import numpy as np\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-20T23:21:46.619002Z","iopub.execute_input":"2024-12-20T23:21:46.619309Z","iopub.status.idle":"2024-12-20T23:21:46.624151Z","shell.execute_reply.started":"2024-12-20T23:21:46.619284Z","shell.execute_reply":"2024-12-20T23:21:46.623345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-12-20T23:21:46.631321Z","iopub.execute_input":"2024-12-20T23:21:46.631550Z","iopub.status.idle":"2024-12-20T23:21:46.635231Z","shell.execute_reply.started":"2024-12-20T23:21:46.631528Z","shell.execute_reply":"2024-12-20T23:21:46.634361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2024-12-20T23:21:46.658326Z","iopub.execute_input":"2024-12-20T23:21:46.658546Z","iopub.status.idle":"2024-12-20T23:21:52.199289Z","shell.execute_reply.started":"2024-12-20T23:21:46.658525Z","shell.execute_reply":"2024-12-20T23:21:52.198347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green> Set Paths</font>","metadata":{}},{"cell_type":"code","source":"home = '/kaggle/input/landscape-image-colorization/landscape Images/'\ntotal_images = len(os.listdir(home+'color'))","metadata":{"execution":{"iopub.status.busy":"2024-12-20T23:21:52.201091Z","iopub.execute_input":"2024-12-20T23:21:52.201430Z","iopub.status.idle":"2024-12-20T23:21:52.234117Z","shell.execute_reply.started":"2024-12-20T23:21:52.201387Z","shell.execute_reply":"2024-12-20T23:21:52.233493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green> Set up Training and Test Indices </font>\n> 80% images are used for training","metadata":{}},{"cell_type":"code","source":"random_indices = random.sample(list(range(total_images)),total_images)\ntrain_nums = round(total_images*0.8)\ntrain_indices = random_indices[:train_nums]\ntest_indices = random_indices[train_nums:]\nlen(train_indices), len(test_indices)","metadata":{"execution":{"iopub.status.busy":"2024-12-20T23:21:52.235395Z","iopub.execute_input":"2024-12-20T23:21:52.235619Z","iopub.status.idle":"2024-12-20T23:21:52.246359Z","shell.execute_reply.started":"2024-12-20T23:21:52.235597Z","shell.execute_reply":"2024-12-20T23:21:52.245492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green> Encoder Dataset </font>\n> Class takes input of training and test indices and creates datasets accordingly\n> Transformation of images are done inside dataset class only\n\n### <font color = orange> Steps </font>\n - read image\n - repeat grayscale image channel 3 times to create a prototype RGB image. Later the two other dimensions will be thrown away. This step is just to cconvert RGB image to LAB color space\n - divide RGB image by 255 to make values between (0,1)\n - Output of rgb2lab() provides L in range of (0,100), a in range of (-128(Green), 127(Red)) and b in range of (-128(Blue), 127(Yellow)). Hence tensor([0,128,128]) is added to each of the dimension followed by normalizing by tensor([100,255,255]).\n - Take L channel from GrayScale image to predict a,b Channel from Color Image\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os                           # For handling file and directory paths\nimport pandas as pd                 # For working with tabular data (not used in this code)\nfrom torchvision.io import read_image  # For reading image files as PyTorch tensors\nfrom torch.utils.data import Dataset   # Base class for creating custom datasets in PyTorch\nimport torch.nn.functional as nnFunctions     # For neural network functions like interpolation\nimport torch                        # PyTorch library for tensors and operations\nfrom skimage.color import rgb2lab, lab2rgb  # For converting between RGB and LAB color spaces\n\n# Define a custom dataset class for handling grayscale and color images\nclass EncoderDataset(Dataset):\n    def __init__(self, indices, img_dir, transform=None):\n        \"\"\"\n        Initialize the dataset with image indices, directory paths, and optional transforms.\n        \"\"\"\n        self.img_dir = img_dir                      # Base directory containing images\n        self.transform = transform                  # Transformations to apply to images (if any)\n        self.img_indices = indices                  # List of image indices\n        self.gray_path = img_dir + 'gray/'          # Directory containing grayscale images\n        self.color_path = img_dir + 'color/'        # Directory containing color images\n    \n    def __len__(self):\n        \"\"\"\n        Return the total number of images in the dataset.\n        \"\"\"\n        return len(self.img_indices)                # Return the length of the indices list\n        \n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve a single item (image and label) by index.\n        \"\"\"\n        img_name = str(idx) + '.jpg'                # Construct the image filename\n        \n        # Read the grayscale image\n        image = read_image(self.gray_path + img_name)  # Load grayscale image as a tensor ##read_image this function imported from torch\n        image = image.unsqueeze(0)                  # Add a batch dimension (for interpolate function below)\n        image = nnFunctions.interpolate(image, (160, 160))    # Resize the image to 160x160\n        image = image.squeeze(0)                    # Remove the batch dimension\n        image = image.repeat(3, 1, 1)  # Duplicate the grayscale channel directly to create 3 channels ## to make pseudo-RGB\n\n        random_number = random.random()\n         \n        if (random_number > 0.5) :\n            if self.transform:\n              image = self.transform(image)\n   \n        # Read the color image\n        label = read_image(self.color_path + img_name)  # Load color image as a tensor\n        label = label.unsqueeze(0)                  # Add a batch dimension (for interpolation)\n        label = nnFunctions.interpolate(label, (160, 160))    # Resize the image to 160x160\n        label = label.squeeze(0)                    # Remove the batch dimension\n        \n        \n        # Convert images to LAB color space and normalize\n        #####---- we rearrange and change scale to be compatible with 'rgb2lab' function ----#####\n        #• We permute to [Height, Width, Channels] and divide by 255 so pixels range from 0 to 1.\n        #• rgb2lab(...) converts from RGB color space to LAB.\n        #• Then we wrap the result in torch.tensor(...) to convert it back to a PyTorch tensor.\n        \n        \n        image = torch.tensor( rgb2lab(image.permute(1, 2, 0) / 255) )  # Convert grayscale to LAB\n        label = torch.tensor( rgb2lab(label.permute(1, 2, 0) / 255) )  # Convert color to LAB\n        #print(f'image before normalization = {image}')\n        \n        # Normalize LAB values to be in the range [0, 1]\n        image = (image + torch.tensor([0, 128, 128])) / torch.tensor([100, 255, 255])\n        label = (label + torch.tensor([0, 128, 128])) / torch.tensor([100, 255, 255])\n        #print(f'image after normalization = {image}')\n        \n        # Rearrange dimensions back to [Channels, Height, Width]\n        image = image.permute(2, 0, 1)\n        label = label.permute(2, 0, 1)\n        \n        # Extract the L channel (grayscale) from the input image\n        image = image[:1, :, :]                     # Use only the L channel as input\n        \n        # Extract the a and b channels (color information) from the label\n        label = label[1:, :, :]                     # Use only the a and b channels as the target\n        \n        return image, label                         # Return the processed input (L) and label (a, b)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-20T23:22:40.814418Z","iopub.execute_input":"2024-12-20T23:22:40.814713Z","iopub.status.idle":"2024-12-20T23:22:40.825668Z","shell.execute_reply.started":"2024-12-20T23:22:40.814687Z","shell.execute_reply":"2024-12-20T23:22:40.824701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),  # horizontal flipping\n    transforms.RandomRotation(degrees=15),  # Random rotation by +/- 15 degrees\n    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Adjust color properties\n    transforms.ToTensor()]\n)\n'''\ntest_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor()]\n)\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T23:27:39.023105Z","iopub.execute_input":"2024-12-20T23:27:39.023381Z","iopub.status.idle":"2024-12-20T23:27:39.029461Z","shell.execute_reply.started":"2024-12-20T23:27:39.023357Z","shell.execute_reply":"2024-12-20T23:27:39.028655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = EncoderDataset(indices = train_indices,img_dir = home, transform = None)\ntest_dataset = EncoderDataset(indices = test_indices,img_dir = home, transform = None)\ntrain_dataloader = DataLoader(train_dataset,batch_size=16,shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:24:46.476498Z","iopub.execute_input":"2024-12-21T01:24:46.476798Z","iopub.status.idle":"2024-12-21T01:24:46.481256Z","shell.execute_reply.started":"2024-12-21T01:24:46.476766Z","shell.execute_reply":"2024-12-21T01:24:46.480438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green> Create Datasets </font>","metadata":{"execution":{"iopub.status.busy":"2021-08-16T06:37:52.253811Z","iopub.execute_input":"2021-08-16T06:37:52.254206Z","iopub.status.idle":"2021-08-16T06:37:52.258229Z","shell.execute_reply.started":"2021-08-16T06:37:52.254171Z","shell.execute_reply":"2021-08-16T06:37:52.257075Z"}}},{"cell_type":"markdown","source":"## <font color = green> Sample Image </font>\n    - Remember we have 1 color channel input for image and 2 color channel output of label","metadata":{}},{"cell_type":"code","source":"img,label = next(iter(train_dataloader))\nsample_image,sample_label = img[0], label[0]\nprint(sample_image.shape, sample_label.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T23:22:51.208416Z","iopub.execute_input":"2024-12-20T23:22:51.208746Z","iopub.status.idle":"2024-12-20T23:22:51.536872Z","shell.execute_reply.started":"2024-12-20T23:22:51.208712Z","shell.execute_reply":"2024-12-20T23:22:51.536057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img,label = next(iter(train_dataloader))\nimg[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T23:23:37.065618Z","iopub.execute_input":"2024-12-20T23:23:37.065944Z","iopub.status.idle":"2024-12-20T23:23:37.424438Z","shell.execute_reply.started":"2024-12-20T23:23:37.065913Z","shell.execute_reply":"2024-12-20T23:23:37.423660Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green> Visualize Image </font>\n    Grayscale and Color images produce same L (intensity) channel pixel values. Here first training image is plotted which is grayscale image. Followed by, from Label(Color image) 'a' and 'b' channels are plotted. Finally Grayscale image L channel and Color image 'a', 'b' channels are concatenated followed by lab2rgb() providing RGB channel output (plotted at the last)","metadata":{}},{"cell_type":"code","source":"random_number = random.randint(1, 16)\nsample_image,sample_label = img[random_number], label[random_number]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T23:26:52.561781Z","iopub.execute_input":"2024-12-20T23:26:52.562070Z","iopub.status.idle":"2024-12-20T23:26:52.565825Z","shell.execute_reply.started":"2024-12-20T23:26:52.562046Z","shell.execute_reply":"2024-12-20T23:26:52.564975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nrandom_number = random.randint(0, 15)\nsample_image,sample_label = img[random_number], label[random_number]\n\nfig = plt.figure(figsize=(16,16))\nplt.subplot(441)\nplt.imshow(sample_image.permute(1,2,0),cmap='gray')\nplt.title('Image - Gray Scale \"L\" Channel')\nplt.subplot(442)\nplt.imshow(sample_label.permute(1,2,0)[:,:,0],cmap='Greens')\nplt.title('Lab Image - \"a\" Channel')\n\nplt.subplot(443)\nplt.imshow(sample_label.permute(1,2,0)[:,:,1],cmap='Blues')\nplt.title('Lab Image - \"b\" Channel')\n\nplt.subplot(444)\ncolor_image = torch.cat((sample_image,sample_label),dim=0).permute(1,2,0)\ncolor_image = color_image * torch.tensor([100,255,255]) -torch.tensor([0,128,128])\ncolor_image = lab2rgb(color_image)\nplt.imshow(color_image)\nplt.title('RGB Image')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:24:58.939182Z","iopub.execute_input":"2024-12-21T01:24:58.939576Z","iopub.status.idle":"2024-12-21T01:24:59.342028Z","shell.execute_reply.started":"2024-12-21T01:24:58.939541Z","shell.execute_reply":"2024-12-21T01:24:59.341185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green> Convolution Autoencoder </font>\n    - Network takes 1 channel input\n    - No maxpool layer is added at the encoder step\n    - Transpose Convolution takes place in decoder step\n    - Decoder outputs are concatenated with encoder output of the same layer\n    - Dropout layer added in decoder layer only\n    - Final layer CNN is the converging layer which outputs 2 channels","metadata":{}},{"cell_type":"code","source":"# Import PyTorch modules for defining neural network layers and functions\nimport torch.nn as nn            # For defining layers like Conv2d, ConvTranspose2d, etc.\nimport torch.nn.functional as F  # For activation functions like ReLU\n\n# Define the Convolutional Autoencoder class\nclass ConvAutoencoder(nn.Module):\n    def __init__(self):  # Constructor to define the layers\n        super(ConvAutoencoder, self).__init__()  # Call the parent class (nn.Module) constructor\n        \n        ## Encoder layers ##\n        # First convolution layer: input channel = 1 (grayscale), output channels = 64\n        # Kernel size = 3x3, stride = 1, padding = 1 (output size is preserved)\n        self.conv1 = nn.Conv2d(1, 64, 3, stride=1, padding=1)\n        \n        # Second convolution layer: input channels = 64, output channels = 64\n        # Stride = 2 reduces the spatial dimensions by half\n        self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n        \n        # Third convolution layer: input channels = 64, output channels = 128\n        # Stride = 2 further reduces spatial dimensions by half\n        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n        \n        # Fourth convolution layer: input channels = 128, output channels = 256\n        # Stride = 2 reduces spatial dimensions again\n        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n        \n        # Max-pooling layer (not used in this model but defined for possible use)\n        self.pool = nn.MaxPool2d(2, 2)  # Pooling reduces spatial dimensions by a factor of 2\n\n        ## Decoder layers ##\n        # First transpose convolution layer: upsampling the feature map\n        # Input channels = 256, output channels = 128\n        # Stride = 2 increases spatial dimensions by a factor of 2\n        self.t_conv1 = nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1)\n        \n        # Second transpose convolution layer: upsampling and concatenating with skip connection\n        # Input channels = 256 (128 + 128 from skip connection), output channels = 64\n        self.t_conv2 = nn.ConvTranspose2d(256, 64, 3, stride=2, padding=1, output_padding=1)\n        \n        # Third transpose convolution layer: upsampling\n        # Input channels = 128 (64 + 64 from skip connection), output channels = 128\n        self.t_conv3 = nn.ConvTranspose2d(128, 128, 3, stride=2, padding=1, output_padding=1)\n        \n        # Fourth transpose convolution layer: refining the output\n        # Input channels = 192 (128 + 64 from skip connection), output channels = 15\n        self.t_conv4 = nn.ConvTranspose2d(192, 15, 3, stride=1, padding=1)\n        \n        # Dropout layer: randomly disables 20% of neurons during training to reduce overfitting\n        self.dropout = nn.Dropout(0.2)\n        \n        # Final convolution layer: reduces output channels to 2 (for 'a' and 'b' channels)\n        self.converge = nn.Conv2d(16, 2, 3, stride=1, padding=1)\n\n    def forward(self, x):  # Defines the forward pass of the network\n        # Encoder part\n        x1 = F.relu(self.conv1(x))  # Pass through first convolution and ReLU activation\n        x2 = F.relu(self.conv2(x1)) # Pass through second convolution and ReLU activation\n        x3 = F.relu(self.conv3(x2)) # Pass through third convolution and ReLU activation\n        x4 = F.relu(self.conv4(x3)) # Pass through fourth convolution and ReLU activation\n        \n        # Decoder part\n        xd = F.relu(self.t_conv1(x4))  # Upsample the feature map\n        xd = torch.cat((xd, x3), dim=1)  # Concatenate with skip connection from x3\n        xd = self.dropout(xd)  # Apply dropout\n        \n        xd = F.relu(self.t_conv2(xd))  # Upsample again\n        xd = torch.cat((xd, x2), dim=1)  # Concatenate with skip connection from x2\n        xd = self.dropout(xd)  # Apply dropout\n        \n        xd = F.relu(self.t_conv3(xd))  # Upsample\n        xd = torch.cat((xd, x1), dim=1)  # Concatenate with skip connection from x1\n        xd = self.dropout(xd)  # Apply dropout\n        \n        xd = F.relu(self.t_conv4(xd))  # Upsample to match input size\n        xd = torch.cat((xd, x), dim=1)  # Concatenate with the original input (L channel)\n        \n        # Final layer to predict 'a' and 'b' channels\n        x_out = F.relu(self.converge(xd))\n        return x_out  # Output the predicted 'a' and 'b' channels\n\n# Instantiate the model\nmodel = ConvAutoencoder()\n\n# Print the model architecture to verify\nprint(model)\n\n# Move the model to GPU (if available)\nmodel = model.to('cuda')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:25:08.422173Z","iopub.execute_input":"2024-12-21T01:25:08.422459Z","iopub.status.idle":"2024-12-21T01:25:08.448370Z","shell.execute_reply.started":"2024-12-21T01:25:08.422433Z","shell.execute_reply":"2024-12-21T01:25:08.447437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model,input_size=(1,160,160))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T23:38:51.494255Z","iopub.execute_input":"2024-12-20T23:38:51.494574Z","iopub.status.idle":"2024-12-20T23:38:51.509949Z","shell.execute_reply.started":"2024-12-20T23:38:51.494543Z","shell.execute_reply":"2024-12-20T23:38:51.509174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# specify loss function\ncriterion = nn.MSELoss()\n\n# specify optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:25:13.160577Z","iopub.execute_input":"2024-12-21T01:25:13.160870Z","iopub.status.idle":"2024-12-21T01:25:13.165378Z","shell.execute_reply.started":"2024-12-21T01:25:13.160842Z","shell.execute_reply":"2024-12-21T01:25:13.164554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the number of epochs for training the model\nn_epochs = 1  # The model will be trained for 30 iterations over the entire dataset\n\n# Lists to store the training and testing losses for each epoch\ntrain_losses = []  # Will store average training loss for each epoch\ntest_losses = []   # Will store average testing loss for each epoch\n\n# Loop over the number of epochs\nfor epoch in range(1, n_epochs + 1):  # Epoch counter starts from 1 to n_epochs (inclusive)\n    train_loss = 0.0  # Initialize the cumulative training loss for the current epoch to 0\n    # Loop over batches in the training data\n    for data in tqdm(train_dataloader):  # tqdm provides a progress bar for visualization\n        images, labels = data           # Extract images (inputs) and labels (targets) from the batch\n        # Move images and labels to the GPU (if available)\n        images = images.float().to('cuda')  # Convert images to float and move to GPU\n        labels = labels.float().to('cuda')  # Convert labels to float and move to GPU\n        optimizer.zero_grad()  # Reset gradients for the optimizer\n        outputs = model(images)  # Forward pass: Compute predictions from the model\n        loss = criterion(outputs, labels)  # Compute the loss between predictions and targets\n        loss.backward()  # Backward pass: Compute gradients of loss w.r.t model parameters\n        optimizer.step()  # Update model parameters based on computed gradients\n        # Accumulate training loss (scaled by batch size)\n        train_loss += loss.item() * images.size(0)\n    # Compute average training loss for the epoch\n    train_loss = train_loss / len(train_dataloader.dataset)\n    train_losses.append(train_loss)  # Append the average training loss to the list\n\n    # Print the training loss for the current epoch\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n        epoch, \n        train_loss\n    ))\n    \n    # Initialize cumulative testing loss for the current epoch\n    test_loss = 0\n    #Turn off gradient computation for validation/testing to save memory and speed up computation\n    with torch.no_grad():  \n        model.eval()  # Set the model to evaluation mode (disables dropout, batch normalization updates)\n        # Loop over batches in the testing data\n        for images, labels in test_dataloader:\n            # Move images and labels to the GPU (if available)\n            images, labels = images.to('cuda'), labels.to('cuda')\n            output = model(images)  # Forward pass: Compute predictions from the model\n            loss = criterion(output, labels)  # Compute the loss between predictions and targets\n            # Accumulate testing loss (scaled by batch size)\n            test_loss += loss.item() * images.size(0)\n\n    model.train()  # Switch the model back to training mode (enables dropout, batch normalization updates)\n\n    # Compute average testing loss for the epoch\n    test_loss = test_loss / len(test_dataloader.dataset)\n    test_losses.append(test_loss)  # Append the average testing loss to the list\n\n    # Print the testing loss for the current epoch\n    print(\"Test Loss: {:.3f}.. \".format(test_loss))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:25:14.937724Z","iopub.execute_input":"2024-12-21T01:25:14.938056Z","iopub.status.idle":"2024-12-21T01:27:42.917721Z","shell.execute_reply.started":"2024-12-21T01:25:14.938028Z","shell.execute_reply":"2024-12-21T01:27:42.916927Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font color = green>Visualize Prediction </font>\n - While the model is trained as per expectation and producing colors which are not way-off, this model can be set for a base line model. ","metadata":{}},{"cell_type":"code","source":"## Train Data\ni = 0  # Initialize a counter to keep track of the number of images to process\n\n# Loop to process and visualize 20 images\nwhile i < 20:  # Run the loop until 20 images have been processed\n    # Fetch a batch of images and labels from the training dataloader\n    test_img, test_label = next(iter(train_dataloader))\n\n    # Pass the first image from the batch through the model for prediction\n    # Convert the image to float, move it to GPU, and reshape to match the model's input shape\n    pred = model.forward(test_img[0].float().cuda().view(1, 1, 160, 160))\n\n    # Concatenate the grayscale channel (L) with the predicted color channels (a, b)\n    # to form a complete LAB image\n    lab_pred = torch.cat((test_img[0].view(1, 160, 160), pred[0].cpu()), dim=0)\n\n    # Inverse scaling of the LAB image to bring it back to its original range\n    # Rearranges the dimensions of lab_pred from [Channels, Height, Width] to [Height, Width, Channels].\n    # Scales each channel of the LAB image back to its original range:\n    # Shifts the A and B channels back to their original center point\n    lab_pred_inv_scaled = lab_pred.permute(1, 2, 0) * torch.tensor([100, 255, 255]) - torch.tensor([0, 128, 128])\n\n    # Convert the LAB image to RGB format for visualization\n    # Used to detach the tensor from the computation graph in PyTorch.\n    # Ensures that this operation doesn’t compute gradients \n    rgb_pred = lab2rgb(lab_pred_inv_scaled.detach().numpy())\n\n    # Create a new figure for displaying the images\n    fig = plt.figure(figsize=(10, 10))\n\n    # Display the original grayscale image (L channel)\n    plt.subplot(221)  # Position the subplot in a 2x2 grid at position 1\n    plt.imshow(test_img[0].permute(1, 2, 0), cmap='gray')  # Rearrange dimensions for display\n    plt.title('GrayScale Image')  # Add title to the subplot\n\n    # Display the predicted colorized RGB image\n    plt.subplot(222)  # Position the subplot in a 2x2 grid at position 2\n    plt.imshow(rgb_pred)  # Show the predicted RGB image\n    plt.title('Predicted Color Image')  # Add title to the subplot\n\n    # Show the figure with the two subplots\n    plt.show()\n\n    i += 1  # Increment the counter to process the next image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:27:42.919311Z","iopub.execute_input":"2024-12-21T01:27:42.919668Z","iopub.status.idle":"2024-12-21T01:27:52.912487Z","shell.execute_reply.started":"2024-12-21T01:27:42.919631Z","shell.execute_reply":"2024-12-21T01:27:52.911635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##Test Data\ni=0\nwhile i<20:\n    test_img,test_label = next(iter(test_dataloader))\n    pred = model.forward(test_img[0].float().cuda().view(1,1,160,160))\n    lab_pred = torch.cat((test_img[0].view(1,160,160),pred[0].cpu()),dim=0)\n    lab_pred_inv_scaled = lab_pred.permute(1,2,0) * torch.tensor([100,255,255]) - torch.tensor([0,128,128])\n    rgb_pred = lab2rgb(lab_pred_inv_scaled.detach().numpy())\n    fig = plt.figure(figsize=(10,10))\n    plt.subplot(221)\n    plt.imshow(test_img[0].permute(1,2,0),cmap='gray')\n    plt.title('GrayScale Image')\n    plt.subplot(222)\n    plt.imshow(rgb_pred)\n    plt.title('Predicted Color Image')\n    plt.show()\n    i+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T01:27:52.914154Z","iopub.execute_input":"2024-12-21T01:27:52.914502Z","iopub.status.idle":"2024-12-21T01:28:13.803221Z","shell.execute_reply.started":"2024-12-21T01:27:52.914466Z","shell.execute_reply":"2024-12-21T01:28:13.802423Z"}},"outputs":[],"execution_count":null}]}